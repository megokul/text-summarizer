# Parameters for data transformation
transformation_params:
  data_split:
    train_size: 0.8
    val_size: 0.1
    test_size: 0.1
    random_state: 42
    stratify: false

  tokenizer:
    pretrained_model_name: google/pegasus-cnn_dailymail
    max_input_length: 1024
    max_target_length: 128

# Model training arguments
model_trainer:
  training_arguments:
    num_train_epochs: 3
    warmup_steps: 500
    per_device_train_batch_size: 2
    per_device_eval_batch_size: 2
    weight_decay: 0.01
    logging_steps: 10
    evaluation_strategy: steps
    eval_steps: 500
    save_steps: 5000
    gradient_accumulation_steps: 8
    learning_rate: 3e-5
    fp16: true

# MLflow tracking settings
tracking:
  mlflow:
    enabled: true
    experiment_name: TextSummarizationExperiment
    registry_model_name: PegasusSummarizerModel
    metrics_to_log:
      - rouge1
      - rouge2
      - rougeL
      - rougeLsum
    log_trials: false

# Model evaluation metrics
model_evaluation:
  metrics:
    - rouge1
    - rouge2
    - rougeL
    - rougeLsum
    - bleu
    - meteor
